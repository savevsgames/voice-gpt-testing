


import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Access the secrets
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
ELEVENLABS_API_KEY = os.getenv("ELEVENLABS_API_KEY")

print(f"OpenAI Key: {OPENAI_API_KEY[:5]}... (Hidden for security)")
print(f"11Labs Key: {ELEVENLABS_API_KEY[:5]}... (Hidden for security)")






from openai import OpenAI
client = OpenAI()





import sounddevice as sd
import numpy as np
import wave

# List available audio devices
devices = sd.query_devices()
input_devices = [d for i, d in enumerate(devices) if d["max_input_channels"] > 0]

# Print available input devices
print("\nAvailable Input Devices:")
for i, device in enumerate(input_devices):
    print(f"{i}: {device['name']} (Max Channels: {device['max_input_channels']})")

# Prompt user for device selection
device_index = int(input("\nEnter the number of the device you want to use: "))
selected_device = input_devices[device_index]

# Set the selected device
sd.default.device = selected_device["index"]
channels = min(selected_device["max_input_channels"], 2)  # Use max available channels, up to 2

print(f"\nUsing device: {selected_device['name']} with {channels} channels.\n")

# Recording parameters
duration = 10  # seconds
sample_rate = 44100

print("Recording...")
audio_data = sd.rec(int(sample_rate * duration), samplerate=sample_rate, channels=channels, dtype=np.int16)
sd.wait()
print("Recording complete.")

# Save to a WAV file
filename = "user_recording.wav"
with wave.open(filename, 'wb') as wf:
    wf.setnchannels(channels)
    wf.setsampwidth(2)
    wf.setframerate(sample_rate)
    wf.writeframes(audio_data.tobytes())

print(f"Saved recording as {filename}")






def transcribe_audio(audio_path):
    with open(audio_path, "rb") as audio_file:
        transcript = client.audio.transcriptions.create(
            model="whisper-1",
            file=audio_file
        )
    return transcript.text

transcribed_text = transcribe_audio("user_recording.wav")
print("Transcribed Text:", transcribed_text)





def get_gpt_response(prompt):
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )
    return response["choices"][0]["message"]["content"]

response_text = get_gpt_response(transcribed_text)
print("GPT Response:", response_text)





import requests

ELEVENLABS_API_KEY = "your-11labs-api-key"

def text_to_speech_11labs(text, voice_id="Rachel"):
    url = f"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}"
    headers = {"xi-api-key": ELEVENLABS_API_KEY, "Content-Type": "application/json"}
    data = {"text": text, "model_id": "eleven_monolingual_v1"}

    response = requests.post(url, headers=headers, json=data)
    if response.status_code == 200:
        with open("output_speech.mp3", "wb") as f:
            f.write(response.content)
        print("Generated Speech: output_speech.mp3")
    else:
        print("Error:", response.text)

text_to_speech_11labs(response_text)






import IPython.display as display

# Display a download link in Jupyter Notebook
def download_audio(filename="output_speech.mp3"):
    return display.FileLink(filename)

download_audio()

